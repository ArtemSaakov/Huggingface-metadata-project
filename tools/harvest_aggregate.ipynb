{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for harvesting the training data\n",
    "# all of the modules and global variables are defined here\n",
    "from sickle import Sickle\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "# destination for fetched docs, goes to my large SSD in this case\n",
    "# change internal strings to match your system and needs\n",
    "DEST_LARGE = Path(\"/mnt/d/data-large/\").absolute()\n",
    "# stored locally if size is not a concern\n",
    "DEST_SMALL = Path().cwd().absolute() / \"datasets/\"\n",
    "# alternative local directory\n",
    "DEST_SMALL_ALT = Path().cwd().absolute() / \"datasets-alt/\"\n",
    "# general repository for pulling data OAI-PMH-compliant\n",
    "WORKING_REPO = \"https://oai.datacite.org/oai/\"\n",
    "# umich OAI-PMH repository for deepblue/dspace\n",
    "UMICH_REPO = \"https://deepblue.lib.umich.edu/dspace-oai/request/\"\n",
    "# set identifier for library\n",
    "BHL_SET = \"com_2027.42_65133\"\n",
    " # collection of other endpoints I utilized\n",
    "ENDPOINT_COLLECTION = {\n",
    "    \"IJHS\": \"https://www.ijhsonline.com/index.php/IJHS/oai\",\n",
    "    \"IJESS\": \"https://journalkeberlanjutan.com/index.php/ijesss/oai\",\n",
    "    \"Medan\": \"https://jurnal.medanresourcecenter.org/index.php/ICI/oai?\",\n",
    "    \"YWNFR\": \"https://jurnal.ywnr.org/index.php/cfabr/oai\",\n",
    "    \"UTOR\": \"https://symposia.library.utoronto.ca/index.php/symposia/oai\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def harvester(*args):\n",
    "    dest = url = metadata_prefix = max_files = dataset = None\n",
    "\n",
    "    # this try/except essentially tries to populate five arguments, and then only\n",
    "    # four if it fails to unpack 5\n",
    "    try:\n",
    "        dest, url, metadata_prefix, max_files, dataset = args\n",
    "    except ValueError:\n",
    "        dest, url, metadata_prefix, max_files = args\n",
    "    if isinstance(dest, str):\n",
    "        dest = Path(dest)\n",
    "    if not dest.exists():\n",
    "        dest.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    sckl = Sickle(url)\n",
    "    records = sckl.ListRecord(metadataPrefix=metadata_prefix, set=dataset)\n",
    "    filecount = 0\n",
    "    errorcount = 0\n",
    "    try:\n",
    "        for rec in records:\n",
    "            id = rec.header.identifier.replace(\":\", \"_\").replace(\"/\", \"_\")\n",
    "            try:\n",
    "                metadata_json = json.dumps(rec.metadata, indent=2)\n",
    "                filepath = f\"{dest / Path(id)}.json\"\n",
    "                with open(filepath, \"w\") as f:\n",
    "                    f.write(metadata_json)\n",
    "                print(f\"wrote #{filecount}: {id}\")\n",
    "                filecount += 1\n",
    "            except (AttributeError, TypeError) as e:\n",
    "                print(f\"skipped {id} due to json incompatibility: {e}\")\n",
    "                errorcount += 1\n",
    "                continue\n",
    "            if filecount >= int(max_files):\n",
    "                print(f\"Final filecount: {filecount}\")\n",
    "                print(f\"Final errorcount: {errorcount}\")\n",
    "                return\n",
    "    except IndexError as e:\n",
    "        raise Exception(\n",
    "            f\"Error: {e} - there may be an issue with your call to the data source\"\n",
    "        )\n",
    "\n",
    "\n",
    "def records_aggregator(records_path: str | Path) -> dict:\n",
    "\n",
    "    if isinstance(records_path, str):\n",
    "        records_path = Path(records_path)\n",
    "    error_count = 0\n",
    "    proc = {}\n",
    "    rec = None\n",
    "\n",
    "    for file in records_path.glob(\"*.json\"):\n",
    "        try:\n",
    "            with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "                rec = json.load(f)\n",
    "            for k in rec.keys():\n",
    "                if k not in proc.keys() and k == \"description\":\n",
    "                    proc[k] = [\n",
    "                        v for v in rec[k] if v and not v.startswith(\"http\")\n",
    "                    ]\n",
    "                elif k not in proc.keys():\n",
    "                    proc[k] = rec[k]\n",
    "                elif rec[k]:\n",
    "                    for v in rec[k]:\n",
    "                        if v not in proc[k]:\n",
    "                            # to skip urls in umich descriptions, since they're more administrative\n",
    "                            if (\n",
    "                                \"umich\" in file.name\n",
    "                                and k == \"description\"\n",
    "                                and v\n",
    "                                and v.startswith(\"http\")\n",
    "                            ):\n",
    "                                continue\n",
    "                            proc[k].append(v)\n",
    "        except (json.JSONDecodeError, AttributeError, TypeError) as e:\n",
    "            print(\n",
    "                f\"skipped {file} due to json incompatibility or similar issue\"\n",
    "            )\n",
    "            print(f\"Error code: {e}\")\n",
    "            error_count += 1\n",
    "\n",
    "    print(f\"Errors encountered: {error_count}\")\n",
    "    return proc\n",
    "\n",
    "\n",
    "def flatten_aggregated_data(filepath: str | Path) -> list:\n",
    "    \"\"\"\n",
    "    Flatten aggregated metadata into a list of training instances.\n",
    "\n",
    "    This function reads an aggregated JSON file of metadata specified by the filepath.\n",
    "    The file should contain a single JSON object where each key is a metadata field\n",
    "    (e.g., \"description\") and its value is a list of corresponding metadata values.\n",
    "    The function transforms this object into a flat list of dictionaries where each\n",
    "    dictionary represents a training instance with two keys:\n",
    "      - \"text\": a non-empty, stripped metadata value.\n",
    "      - \"label\": the metadata field associated with the value.\n",
    "\n",
    "    Args:\n",
    "        filepath (str or Path): The path to the aggregated data JSON file.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries each with keys \"text\" and \"label\".\n",
    "\n",
    "    Raises:\n",
    "        Exception: If the file cannot be parsed due to JSON decoding errors,\n",
    "                   attribute issues, or type incompatibility.\n",
    "    \"\"\"\n",
    "    if isinstance(filepath, str):\n",
    "        filepath = Path(filepath)\n",
    "\n",
    "    try:\n",
    "        with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "            aggregated_data = json.load(f)\n",
    "\n",
    "        flattened_data = []\n",
    "\n",
    "        # iterate over each field and its list of values.\n",
    "        for field, values in aggregated_data.items():\n",
    "            # for each metadata value in the list, create an individual training instance\n",
    "            # each entry should be a dict with \"label\" and \"text\" keys,\n",
    "            # where label is the metadata field and text is each corresponding value\n",
    "            for value in values:\n",
    "                # this checks if the value is a non-empty string\n",
    "                if isinstance(value, str) and value.strip():\n",
    "                    flattened_data.append(\n",
    "                        {\"text\": value.strip(), \"label\": field}\n",
    "                    )\n",
    "    except (json.JSONDecodeError, AttributeError, TypeError) as e:\n",
    "        raise Exception(\n",
    "            f\"failed due to json incompatibility or similar issue: {e} \"\n",
    "            \"Check the formatting of your aggregated data file. It should be a single JSON object\"\n",
    "        )\n",
    "\n",
    "    return flattened_data\n",
    "\n",
    "\n",
    "def data_integrity_check(data: list, *labels) -> None:\n",
    "    \"\"\"\n",
    "    Quick function to check the training data doesn't have any erroneous labels\n",
    "\n",
    "    Args:\n",
    "        data (list): List of dictionaries containing the training data.\n",
    "\n",
    "        *labels: Labels to check against.\n",
    "    \"\"\"\n",
    "    for i, dict in enumerate(data):\n",
    "        if \"text\" not in dict.keys() or \"label\" not in dict.keys():\n",
    "            print(f\"Error #1 in entry {i}: {dict}\")\n",
    "            continue\n",
    "        if not isinstance(dict[\"text\"], str) or not isinstance(\n",
    "            dict[\"label\"], str\n",
    "        ):\n",
    "            print(f\"Error #2 in entry {i}: {dict}\")\n",
    "            continue\n",
    "        if not dict[\"text\"].strip() or not dict[\"label\"].strip():\n",
    "            print(f\"Error #3 in entry {i}: {dict}\")\n",
    "            continue\n",
    "        if dict[\"label\"] not in labels:\n",
    "            print(f\"Error #4 in entry {i}: {dict}\")\n",
    "            continue\n",
    "        print(f\"#{i} is valid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "pt = Path.cwd().parent / Path(\"lang_codes.xlsx\")\n",
    "\n",
    "langs = pd.read_excel(pt, usecols=[0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dta = \"../aggregate_data_new.json\"\n",
    "\n",
    "with open(dta, \"r\", encoding=\"utf-8\") as f:\n",
    "    dtb = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# harvesting operation\n",
    "# this will call the harvesting function and ask for parameters, or will use the defaults\n",
    "\n",
    "(*args,) = (DEST_SMALL_ALT, ENDPOINT_COLLECTION[\"UTOR\"], \"oai_dc\", 2000)\n",
    "\n",
    "d = args[0]\n",
    "harvester(*args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# aggregation operation\n",
    "# this will take the destination input from the harvesting operation above, saved\n",
    "# as d, and use it as the path to the directory containing the harvested data\n",
    "# the data will be aggregated into one long document, \n",
    "if not d:\n",
    "    raise Exception(\"Need a destination for aggregation\")\n",
    "data_path = d\n",
    "\n",
    "recs = records_aggregator(d)\n",
    "with open(f\"{d}.json\", \"w\") as f:\n",
    "    json.dump(recs, f, indent=2, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternate aggregator for more contextualized training data\n",
    "aggregated_record = \"aggregate_data_new.json\"\n",
    "\n",
    "with open(\"raw_records.json\") as f:\n",
    "    records = json.load(f)\n",
    "\n",
    "examples = []\n",
    "for rec in records:\n",
    "    for field, val in rec.items():\n",
    "        if not val:\n",
    "            continue\n",
    "        snippet  = val if isinstance(val, str) else \" \".join(val)\n",
    "        # build a “context” string of all the *other* fields\n",
    "        context = \" \".join(f\"{k}: {v}\" for k,v in rec.items() if k != field)\n",
    "        examples.append({\n",
    "          \"text\":    snippet,\n",
    "          \"context\": context,\n",
    "          \"label\":   field\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"./aggregate_data_new.json\"\n",
    "# flatten operation\n",
    "try:\n",
    "    flat_data = flatten_aggregated_data(data_path)\n",
    "    with open(\"./flattened_data_bhl_set.json\", \"w\") as f:\n",
    "        json.dump(flat_data, f, indent=2, ensure_ascii=False)\n",
    "except Exception as e:\n",
    "    raise (f\"failed to flatten the aggregated data with the following exception: {e}\")\n",
    "# integrity check operation\n",
    "print(\"Goodbye\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-llm (3.11.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
